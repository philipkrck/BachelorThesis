\chapter{Methodology and Study Design}
This chapter explores the employed methodology for testing both the performance hypothesis $H_P$ and the usability hypothesis $H_U$ (mentioned in Section \ref{section::thesis_objective}). 
The goal is to explicitly show the reasoning for the chosen methods as well as provide specific method implementation details to aid transparancy about the obtained results discussed in Chapter \ref{chapter::results}.\\
Generally, the employed methodology to achieve the research aim (Section \ref{section::thesis_objective}) is a replication of a relevant subset of functionality of an existing iOS app using Flutter. Thereby, the original app acts as a baseline with which the Flutter clone 
can be comparatively evaluated. Based in this comparison, the research question - whether the Flutter framework can match native performance and provide equivalent usability - is answered.
Instead of creating artificial use cases, taking advantage of an existing app provides realistic instances for performance as well as user interface testing.\\
The first section in this chapter (Section \ref{section::feature_selection}) details the decision process for selecting the baseline testing app as well as its feature reduction for further comparison. 
The subsequent two sections (Sections \ref{section::performance_comparison_design} and \ref{section::usability_comparison_design}) explore the specific methods and reasoning for the performance and 
usability comparison respectively.




\section{Baseline App Testing Decision Process} \label{section::feature_selection}
The procedure for choosing the case study app is based on a filtering process of 4 steps (i.e. constraints):
\begin{enumerate}
    \item The app is built and maintained by apploft. \label{item::constraint_one}
    \item The app includes common application \textbf{features}. \label{item::constraint_two}
    \item The app uses modern iOS framework technologies. \label{item::constraint_three}
    \item The app conforms to the human interface guidelines (HIG) by Apple (\cite{Apple2021a}). \label{item::constraint_four}
\end{enumerate}
The reasoning behind selecting the above filtering constraints is detailed in the following paragraphs.

\paragraph*{1. Creation and Maintainance by apploft}\hfill \break
This constraint was opposed on the filtering process such that both a contact person (apploft employee)
is available for code specific questions and the original source may be referenced. Thereby, functionality may be implemented in a similar manner to facilitate comparability between both apps.
E.g. a particular algorithm could be implemented similarly in the Flutter application to produce equal runtime complexities and thus be retracted as a confounding variable 
explaining performance differences.
Furthermore, access to the original source code provides the ability to reduce unnecessary features of the original app for proper comparisons against the Flutter clone.

\paragraph*{2. Inclusion of Common Application Features}\hfill \break
On the one hand, including only common features constrains the tested functionality to fit within the scope of this thesis. On the other hand, it allows for
the ability to generalize the results to a majority of iOS applications\footnote{This assumes that the observed apps for selecting features are indicative of the archetypal iOS app.}.


\paragraph*{3. Use of Modern iOS Frameworks}\hfill \break
Constraining the baseline app to be built with modern iterations of iOS framework technology ensures a fair comparison 
against the replica app built with the recently released Flutter framework.


\paragraph*{4. Conformance to HIG}\hfill \break
Conforming to Apple's HIG ensures the original app looks native to the iOS platform. Building a matching Flutter clone is especially 
interesting regarding the usability hypothesis $H_U$ as it would stretch the realm of possibility for Google's framework.
In addition, providing a recognizable UX for iOS users would keep participants in the usability study (detailed in Section ...) focused on noticing differences instead of 
being distracted by an ambiguous UI.\\
\hfill \break
\hfill \break


Based on the above constraints, a small study was conducted looking at 15 apps developed by apploft (constraint \ref{item::constraint_one}) from 9 different iOS App Store categories. 
The goal is to find common application features (constraint \ref{item::constraint_two}) among them.
As for the purposes of this study, a \textbf{feature} is defined as either 
\begin{enumerate}[label=(\alph*)]
    \item a generalizable UI component which is non-trivial, or \label{item::feature_ui}
    \item an underlying technical attribute influencing the user experience. \label{item::technical_attribute}
\end{enumerate}

Trivial UI components \ref{item::feature_ui} such as buttons or text weren't considered \textbf{features} as they are omnipresent throughout every app.
As for \ref{item::technical_attribute}, a technical attribute has to influence the user experience to be incorporated as the purpose of this thesis is testing Flutter's value as a UI framework (see Section \ref{section::thesis_objective}).
For example, networking can be viewed as a \textbf{feature} if fetched data is displayed via the UI, but is not a \textbf{feature} if the sole purpose of networking within 
an app is to extract analytics data.

Furthermore, a \textbf{feature} had to appear at least twice. The results are summarized in Table (...).

Continuing the filtering process, as per constraint \ref{item::constraint_two} uncommon \textbf{features} are excluded which appear in less than 50\% of the 
observed apps. 
This reduces the list of features to the following (see Table \dots):

\begin{itemize}
    \item Networking
    \item Login/Authentication
    \item Tab navigation\footnote{...}
    \item Stack navigation\footnote{...}
    \item Keyboard interaction
    \item Vertically scrolling collections\footnote{...}
    \item Horizontally scrolling collections\footnote{...}
    \item Webview component\footnote{...} integration
\end{itemize}

Out of the 15 initially tested apps, 5 include all of the above \textbf{features}. Kickdown (see Section ...) is chosen among the remaining 
contestants for the baseline testing app. It was most recently release (Feb 2021) and is therefore built with modern iOS technologies (constraint \ref{item::constraint_three})
and complies to the most recent iteration of the \textbf{Human Interface Guidelines} (constraint \ref{item::constraint_four}).
\textit{@Jan: I believe Kickdown has the smallest code base. Is this worth measuring and including as a fifth constraint?}

\textit{@Jan: Would it be intristing to see a graphic of the filtering process using a funnel as an analogy?}


The login and signup mechanism is removed from the original app for baseline testing. 
In terms of hypotheses evaluation, testing the signup mechanism would yield no further insight as opposed to normal networking, 
textfield and button interaction which is already available in other places of the app.

The Flutter app is implemented as closely as possible to the original application to avoid an indiscriminate comparison as detailed in Section \ref{section::implementation}.

\section{Performance Comparison} \label{section::performance_comparison_design}
The methodology chosen to test the performance hypthesis $H_P$ (Section \ref{section::thesis_objective}) is a quantitative comparison along multiple common profiling metrics.

\subsection{Selected Performance Measurement Variables}
The selected variables for performance measurement include CPU, GPU and memory usage during specific user activities (Section \ref{subsection::measurement_process}).
On the hand, the chosen metrics are the underlying causes of more ephemeral metrics such as page load speed. On the other hand, they can be easily 
measured using software tooling (Section \ref{subsection::profiling_tooling}).

\subsection{Measurment Process} \label{subsection::measurement_process}
The measurement process for the individual metrics is further split into specific \textbf{user actions} which are executed and tested on both the iOS and Flutter app separately:
\begin{itemize}
    \item \textbf{app start:} The app is freshly installed on the test device, opened and measured util the visible postings are loaded.
    \item \textbf{scrolling:} On the postings overview screen, the posting cards are scrolled fully to the bottom and subsequently back to the beginning.
    \item \textbf{detail view:} From the postings overview, a posting is tapped to navigate to the detail view. Afterwards the back button is tapped to navigate back to the overview.
    \item \textbf{image gallery:} The image gallery of a posting is opened from the detail view of a posting and the first 10 images are viewed by swiping.
\end{itemize}
For each \textbf{user action}, the least efficient value for the particular metric is selected and averaged over 5 consecutive tests. Furthermore, 2 testing rounds are devised on separate devices. 
The iPhone 12 and iPhone SE are chosen as the upper and lower bounds of hardware performance respectively. The lower bound is defined in this case as per Apples recommendation to set the 
deployment target to the current operating system version (iOS 14 at time of writing) minus one (iOS 13) which lists the iPhone SE as the oldest supported device (\cite{Apple2021}). iOS 13~is also the
deployment target of the original Kickdown app.
To reduce measurement bias, the device is restarted before each measurement to ensure that all irrelevant background processes are cancelled.

\subsection{Profiling Tools} \label{subsection::profiling_tooling}
\textbf{Xcode Instruments} (\cite{Apple2019}) - a part of the \textbf{Xcode} IDE tool set - are used for profiling the individual metrics. It provides multiple preconfigured
profiling trace instruments.
For the purposes of this thesis, the \textbf{Time Profiler} tool (see Figure ...) is used for CPU, the \textbf{Core Animation} tool (see Figure ...) for GPU, and 
\textbf{Allocations} tool (see Figure ...) for memory usage quantification over time.

\textit{Should the units be explained here or does it make more sense to mention it when discussing the results?}


\subsection{Evaluation Process} \label{subsection::evaluation_process}
To better understand the data gathered, it is subsequently visually depicted using exploratory data analysis (EDA) (\cite{Tukey1977}). 
Furthermore, a statistical ANOVA test is conducted to calculate statistical differences between the two groups (I don't know if I should actually do this - haha).


\section{User Experience Comparison} \label{section::usability_comparison_design}
Coming soon\dots