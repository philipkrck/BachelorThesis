\chapter{Methodology and Study Design}
This chapter explores the employed methodology for testing both the performance hypothesis $H_P$ and the usability hypothesis $H_U$ (mentioned in Section \ref{section::thesis_objective}). 
The goal is to explicitly show the reasoning for the chosen methods as well as provide specific method implementation details to aid transparancy about the obtained results discussed in Chapter \ref{chapter::results}.\\
Generally, the employed methodology to achieve the research aim (Section \ref{section::thesis_objective}) is a replication of a relevant subset of functionality of an existing iOS app using Flutter. Thereby, the original app acts as a baseline with which the Flutter clone 
can be comparatively evaluated. Based in this comparison, the research question - whether the Flutter framework can match native performance and provide equivalent usability - is answered.
Instead of creating artificial use cases, taking advantage of an existing app provides realistic instances for performance as well as user interface testing.\\
The first section in this chapter (Section \ref{section::feature_selection}) details the decision process for selecting the baseline testing app as well as its feature reduction for further comparison. 
The subsequent two sections (Sections \ref{section::performance_comparison_design} and \ref{section::usability_comparison_design}) explore the specific methods and reasoning for the performance and 
usability comparison respectively.




\section{Baseline App Testing Decision Process} \label{section::feature_selection}
The procedure for choosing the case study app is based on a filtering process of 4 steps (i.e. constraints):
\begin{enumerate}
    \item The app is built and maintained by apploft. \label{item::constraint_one}
    \item The app includes common application \textbf{features}. \label{item::constraint_two}
    \item The app uses modern iOS framework technologies. \label{item::constraint_three}
    \item The app conforms to the human interface guidelines (HIG) by Apple (\cite{Apple2021a}). \label{item::constraint_four}
\end{enumerate}
The reasoning behind selecting the above filtering constraints is detailed in the following paragraphs.

\paragraph*{1. Creation and Maintainance by apploft}\hfill \break
This constraint was opposed on the filtering process such that both a contact person (apploft employee)
is available for code specific questions and the original source may be referenced. Thereby, functionality may be implemented in a similar manner to facilitate comparability between both apps.
E.g. a particular algorithm could be implemented similarly in the Flutter application to produce equal runtime complexities and thus be retracted as a confounding variable 
explaining performance differences.
Furthermore, access to the original source code provides the ability to reduce unnecessary features of the original app for proper comparisons against the Flutter clone.

\paragraph*{2. Inclusion of Common Application Features}\hfill \break
On the one hand, including only common features constrains the tested functionality to fit within the scope of this thesis. On the other hand, it allows for
the ability to generalize the results to a majority of iOS applications\footnote{This assumes that the observed apps for selecting features are indicative of the archetypal iOS app.}.


\paragraph*{3. Use of Modern iOS Frameworks}\hfill \break
Constraining the baseline app to be built with modern iterations of iOS framework technology ensures a fair comparison 
against the replica app built with the recently released Flutter framework.


\paragraph*{4. Conformance to HIG}\hfill \break
Conforming to Apple's HIG ensures the original app looks native to the iOS platform. Building a matching Flutter clone is especially 
interesting regarding the usability hypothesis $H_U$ as it would stretch the realm of possibility for Google's framework.
In addition, providing a recognizable UX for iOS users would keep participants in the usability study (detailed in Section ...) focused on noticing differences instead of 
being distracted by an ambiguous UI.\\
\hfill \break
\hfill \break


Based on the above constraints, a small study was conducted looking at 15 apps developed by apploft (constraint \ref{item::constraint_one}) from 9 different iOS App Store categories. 
The goal is to find common application features (constraint \ref{item::constraint_two}) among them.
As for the purposes of this study, a \textbf{feature} is defined as either 
\begin{enumerate}[label=(\alph*)]
    \item a generalizable UI component which is non-trivial, or \label{item::feature_ui}
    \item an underlying technical attribute influencing the user experience. \label{item::technical_attribute}
\end{enumerate}

Trivial UI components \ref{item::feature_ui} such as buttons or text weren't considered \textbf{features} as they are omnipresent throughout every app.
As for \ref{item::technical_attribute}, a technical attribute has to influence the user experience to be incorporated as the purpose of this thesis is testing Flutter's value as a UI framework (see Section \ref{section::thesis_objective}).
For example, networking can be viewed as a \textbf{feature} if fetched data is displayed via the UI, but is not a \textbf{feature} if the sole purpose of networking within 
an app is to extract analytics data.

Furthermore, a \textbf{feature} had to appear at least twice before being added as a result (see
Table [initial table]).

Continuing the filtering process, as per constraint 2 uncommon features - features appearing
in less than 50\% of observed apps - are excluded. This reduces the list of features to the following:


\begin{itemize}
    \item Networking
    \item Login/Authentication
    \item Tab navigation\footnote{...}
    \item Stack navigation\footnote{...}
    \item Keyboard interaction
    \item Vertically scrolling collections\footnote{...}
    \item Horizontally scrolling collections\footnote{...}
    \item Webview component\footnote{...} integration
\end{itemize}

Out of the 15 initially tested apps, 5 include all of the above \textbf{features} (conforming to
constraint 1 and 2) (see Table [table after applying constraint 2]). Kickdown (see Section ...) is chosen among the remaining 
contestants for the baseline testing app. It was most recently release (Feb 2021) and is therefore built with modern iOS technologies (constraint \ref{item::constraint_three})
and complies to the most recent iteration of the \textbf{Human Interface Guidelines} (constraint \ref{item::constraint_four}).
\textit{@Jan: I believe Kickdown has the smallest code base. Is this worth measuring and including as a fifth constraint?}

\textit{@Jan: Would it be intristing to see a graphic of the filtering process using a funnel as an analogy?}


The login and signup mechanism - although a common feature - is removed from the original
app for baseline testing. This is due to the fact that textfield and button interaction as well
as networking is already present in other parts of the app and would yield no further insight
regarding the hypotheses evaluation.
The Flutter app is implemented as closely as possible to the original application to avoid an
asymmetrical comparison as detailed in Section \ref{section::implementation}.

\section{Performance Comparison} \label{section::performance_comparison_design}
The methodology chosen to test the performance hypthesis HP (Section \ref{section::thesis_objective}) is a quantitative
measurement of computational resources during app runtime. Measurements are performed for
specific load conditions (i.e use cases). In the process, the original app acts as an empirical
baseline for testing the Flutter replica against.
Directly benchmarking system resources provides insight whether the Flutter framework consumes
compute resources efficiently under typically imposed load settings. Furthermore, system
benchmarking metrics are the underlying cause of more ephemeral measures for testing the
system load itself, e.g. page load time. In addition, the chosen compute resources (explained
in Subsection ...) are easily measured using software tooling (Subsection ...) which aids the
traceability as well as reproducibility of this particular study methodology.

\subsection{Selected Performance Measurement Variables}
The following paragraphs introduce the selected performance measurement variables. Concretely,
a brief definition is given as well as the reasoning for including the particular metric in
this study with regards to evalutating the performance hypothesis (Section \ref{section::thesis_objective}).

\paragraph*{CPU Utilization}\hfill \break
CPU utilization is defined as the CPU time (Free Software Foundation Inc. 1988) of a task
divided by its overall capacity expressed as a percentage. The CPU as well as its integrated GPU7
are responsible for graphics rendering related computations. Generally, high CPU usage is an
indicator of insufficient processing power of the executed task. Therefore, testing CPU utilization
directly assesses whether Flutterâ€™s framework processing requirements for UI rendering can be
fulfilled given the testing hardware (see Section ...).

\paragraph*{Frames per Second}\hfill \break
Frames per Second (FPS) describes the rate at which the system, and in particular, the GPU
completes rendering individual frames. The FPS rate directly determines the smoothness of UI
animations and transitions (\ref{Google2020}).

\paragraph*{Memory Utilization}\hfill \break
Memory utilization is the percentage of available memory capacity used for a specific task. A
High level of memory usage "[...] affects the performance of actual running tasks, as well as
interactive responsiveness" (\ref{Ljubuncic2015}).

\subsection{Measurment Process} \label{subsection::measurement_process}
The measurement process for the individual metrics is further split into specific user actions
which are executed and tested on both the iOS and Flutter app separately. These were chosen
to test all relevant facets of the app (see Section 4.1) and ensure a necessary load on the system:
\begin{itemize}
    \item \textbf{app start:} The app is freshly installed on the test device, opened and idle until the visible postings are loaded.
    \item \textbf{scrolling:} On the postings overview screen, the posting cards are scrolled fully to the bottom and subsequently back to the beginning.
    \item \textbf{detail view:} From the postings overview, the first posting is tapped to navigate to the detail view. Afterwards the back button is tapped to navigate back to the overview.
    \item \textbf{image gallery:} The image gallery of a posting is opened from the detail view of a posting and the first 10 images are viewed by swiping.
\end{itemize}
For each user action, the average of all values over time is recorded. This process is then
repeated 3 times and averaged. The exact number of experiment repitions was chosen as a
tradeoff between marginal accuracy increase and additional experiment execution time.
Furthermore, 2 testing rounds are devised on separate devices. The iPhone 12 and iPhone
SE are chosen as the upper and lower bounds of hardware performance respectively. The lower
bound is defined in this case as per Apples recommendation to set the deployment target to the
current operating system version (iOS 14 at time of writing) minus one (iOS 13) which lists the
iPhone SE as the oldest supported device (...). iOS 13 is also the deployment
target of the original Kickdown app.
To reduce measurement bias, the device is restarted before each measurement to ensure that all
irrelevant background processes are cancelled.

\subsection{Profiling Tools} \label{subsection::profiling_tooling}
\textbf{Xcode Instruments} (\cite{Apple2019}) - a part of the \textbf{Xcode} IDE tool set - are used for profiling the individual metrics. It provides multiple preconfigured
profiling trace instruments.
For the purposes of this thesis, the \textbf{Time Profiler} tool (see Figure ...) is used for CPU, the \textbf{Core Animation} tool (see Figure ...) for GPU, and 
\textbf{Allocations} tool (see Figure ...) for memory usage quantification over time.


\subsection{Evaluation Process} \label{subsection::evaluation_process}
To better understand the data gathered, it is subsequently examined using exploratory data
analysis (EDA) (\cite{Tukey1977}). \textit{To be continued a bit...}


\section{User Experience Comparison} \label{section::usability_comparison_design}
This Section explores the usability hypothesis evaluation methodology. Specifically, laying out
the procedure to answer the question of whether or not the Flutter framework is capable of
reproducing native iOS application user experiences (see Section 1.3).
Generally, as UX is built for other humans, any evaluation is prone to subjectivity and perception
biases (cf. Tversky and Kahneman 1974).
As the experience is directly dependant upon a sufficiently performing system (e.g. scrolling
fluidity), this particular study uses the results of the performance comparison (5.1) as the basis
for the evaluation of the usability hypothesis HU.
Thereby, using a qualitative study methodology built upon a quantitative comparison may
be conducive to additional insight generation.
Specifically, semi structured expert interviews are conducted to evaluate the baseline application
with the Flutter replica by asking the participants pertinent questions. This has the
advantage of covering predetermined topics relevant to the research question while also allowing
spontaneous discussion possibly leading to inconceived findings.
Furthermore, expert interviews are an especially useful approach for scientific explorations
with no or scant preexisting theory (cf. Bogner et al. 2009).