\chapter{Methodology and Study Design}
This chapter explores the employed methodology for testing both the performance hypothesis $H_P$ and the usability hypothesis $H_U$ (defined in Section \ref{section::thesis_objective}). 
The goal is to explicitly show the reasoning for the chosen methods as well as provide specific method implementation details to aid transparancy about the obtained results discussed in Chapter \ref{chapter::results}.\\
Generally, the employed methodology to achieve the research objective (Section \ref{section::thesis_objective}) is to replicate a relevant subset of functionality of an existing iOS app using Flutter. Thereby, the \textbf{original app} acts as a baseline with which the \textbf{Flutter clone} 
can be comparatively evaluated. Based in this comparison, the research question - whether the Flutter framework can match native performance and provide equivalent usability (Section \ref{section::thesis_objective}) - is answered.
Instead of creating artificial use cases, taking advantage of an existing app provides realistic instances for performance as well as user interface testing.\\
The first section in this chapter (Section \ref{section::feature_selection}) details the decision process for selecting the baseline testing app as well as its feature reduction for further comparison. 
The subsequent two sections (Sections \ref{section::performance_comparison_design} and \ref{section::usability_comparison_design}) explore the specific methods and reasoning for the performance and 
usability comparison respectively.




\section{Baseline App Testing Decision Process} \label{section::feature_selection}
The procedure for choosing the case study app is based on a filtering process of 4 steps (i.e. application of constraints):
\begin{enumerate}
    \item The app is built and maintained by apploft. \label{item::constraint_one}
    \item The app includes common application \textbf{features}. \label{item::constraint_two}
    \item The app uses modern iOS framework technologies. \label{item::constraint_three}
    \item The app conforms to the human interface guidelines (HIG) by Apple (\cite{Apple2021a}). \label{item::constraint_four}
\end{enumerate}
The reasoning behind selecting the above filtering constraints is detailed in the following paragraphs.

\subsection*{Creation and Maintainance by apploft}
This constraint was imposed on the filtering process such that a contact person (apploft
employee) is available for code specific questions.\\
Having a reference to the original source code further provides the ability of implementing the Flutter replica similarly to facilitate comparability with the baseline app.
E.g. a particular algorithm could be implemented similarly in the Flutter application. Thereby, equivalent time and space complexities are produced and 
algorithm implementation can be retracted as a confounding variable.\\
Furthermore, access to the original source code provides the ability to reduce unnecessary features which are irrevalent regarding the hypotheses evaluation. This reduces the complexity of the Flutter replica.

\subsection*{Inclusion of Common Application Features}
The goal of this thesis is to determine whether Flutter is comparable in terms of performance ($H_P$) and usability ($H_U$) for the archetypal mobile app (Section \ref{section::methods}). 
Therefore, only features commonly appearing in iOS apps are considered for find the app for baseline testing app.\\
For the purposes of finding the baseline app, a \textbf{feature} is defined as either
\begin{enumerate}[label=(\alph*)]
    \item a generalizable UI component which is non-trivial, or \label{item::feature_ui}
    \item an underlying technical attribute influencing the user experience. \label{item::technical_attribute}
\end{enumerate}

Trivial UI components \ref{item::feature_ui} such as buttons or text weren't considered \textbf{features} as they are omnipresent throughout every app.
As for \ref{item::technical_attribute}, a technical attribute has to influence the user experience to be incorporated as the purpose of this thesis is testing Flutter's value as a UI framework (see Section \ref{section::thesis_objective}).
For example, networking can be viewed as a \textbf{feature} if fetched data is displayed via the UI, but is not a \textbf{feature} if the sole purpose of networking within 
an app is to extract analytics data.


\subsection*{3. Use of Modern iOS Frameworks}
If this constraint were not applied on the filtering process, old iOS technology could be compared to a modernly built Flutter app. 
Therefore, constraining the baseline app to be built with modern iterations of iOS framework technology ensures a reasonable comparison 
against the replica app.\\


\subsection*{4. Conformance to Human Interface Guidelines}
Conforming to Apple's HIG ensures the original app looks native to the iOS platform.
Since Flutter comes from Google, it probably implements the \textbf{Material Design} (\cite{Google2021}) rather well. Rebuilding an app that conforms to the Apple's design guidelines is the more interesting case.\\
In addition, providing a recognizable UX for iOS users would keep participants in the usability study (detailed in Section \ref{section::usability_comparison_design}) focused on noticing differences instead of 
being distracted by an ambiguous UI.\\
\hfill \break


Based on the above constraints, a small study was conducted looking at 15 apps developed by apploft (constraint \ref{item::constraint_one}) from 9 different iOS App Store categories. 
The features were extracted into Table [initial table] by going through each user interface (constraint \ref{item::constraint_two}).
Furthermore, a feature had to appear at least twice before being added as a result.\\
Continuing the filtering process, as per constraint 2 uncommon features - features appearing
in less than 50\% of observed apps - are excluded. This reduces the list of features to the following:

\begin{itemize}
    \item \textbf{Networking} - Interaction with a remote API.
    \item \textbf{Login/Authentication} - User log in meachanism through a UI.
    \item \textbf{Tab navigation} - UI component to quickly switch between different sections of app (\cite{AppleHIGTabBar2021})
    \item \textbf{Hierarchical navigation} - Screens are opened on top of previous screens using a Stack structure (\cite{AppleHIGNavigation2021}).
    \item \textbf{Keyboard interaction} - A UI for inputing text via a software keyboard.
    \item \textbf{Vertically scrolling collections} - A UI collection of items scrolling vertically.
    \item \textbf{Horizontally scrolling collections} - A UI collection of items scrolling horizontally.
    \item \textbf{Webview component integration} - An integrated UI component in the app displaying web content (\cite{AppleHIGWebViews2021}).
\end{itemize}

Out of the 15 initially tested apps, 5 include all of the above \textbf{features} (conforming to
constraint 1 and 2) (see Table [table after applying constraint 2]). Kickdown (see Section ...) is chosen among the remaining 
contestants for the baseline testing app. It was most recently released (Feb 2021) and is therefore built with modern iOS technologies (constraint \ref{item::constraint_three})
and complies to the most recent iteration of the \textbf{Human Interface Guidelines} (constraint \ref{item::constraint_four}).

% Todo: write about Kickdown having the smallest code base.

% Todo: include the filtering process using a funnel analogy

The login and signup mechanism - although a common \textbf{feature} - is removed from the original
app for baseline testing. This is due to the fact that textfield and button interaction as well
as networking is already present in other parts of the app and would yield no further insight
regarding the hypotheses evaluation.\\
The Flutter app is implemented as closely as possible to the original application to avoid an
asymmetrical comparison as detailed in Section \ref{section::implementation}.

\section{Performance Comparison} \label{section::performance_comparison_design}
The methodology chosen to test the performance hypthesis HP (Section \ref{section::thesis_objective}) is a quantitative
measurement of computational resources during app runtime. Measurements are performed for
specific load conditions (i.e use cases). In the process, the original app acts as an empirical
baseline for testing the Flutter replica against.\\
Directly benchmarking system resources provides insight whether the Flutter framework consumes
compute resources efficiently under typically imposed load settings. Furthermore, system
benchmarking metrics are the underlying cause of more ephemeral measures for testing the
system load itself, e.g. page load time. In addition, the chosen compute resources (explained
in Subsection \ref{subsection::selected_measurement_variables}) are easily measured using software tooling (Subsection ...) which aids the
traceability as well as reproducibility of this particular study methodology.

\subsection{Selected Performance Measurement Variables} \label{subsection::selected_measurement_variables}
The following paragraphs introduce the selected performance measurement variables. Concretely,
a brief definition is given as well as the reasoning for including the particular metric in
this study with regards to evalutating the performance hypothesis (Section \ref{section::thesis_objective}).

\paragraph*{CPU Utilization}\hfill \break
CPU utilization is defined as the CPU time (\cite{FSF1988}) of a task
divided by its overall capacity expressed as a percentage. The CPU as well as its integrated GPU\footnote{The GPU is an integrated chip within the System-on-a-Chip (SoC) architecture (\cite{Martin2001}) for all
iPhone processing units (\cite{WikiChip2020}).}
are responsible for graphics rendering related computations. Generally, high CPU usage is an
indicator of insufficient processing power of the executed task. Therefore, testing CPU utilization
directly assesses whether Flutter’s framework processing requirements for UI rendering can be
fulfilled given the testing hardware (see Subsection \ref{subsection::measurement_process}).

\paragraph*{Frames per Second}\hfill \break
Frames per Second (FPS) describes the rate at which the system, and in particular, the GPU
completes rendering individual frames. The FPS rate directly determines the smoothness of UI
animations and transitions (\cite{Google2020}).

\paragraph*{Memory Utilization}\hfill \break
Memory utilization is the percentage of available memory capacity used for a specific task. A
High level of memory usage "[...] affects the performance of actual running tasks, as well as
interactive responsiveness" (\cite{Ljubuncic2015}).

\subsection{Measurment Process} \label{subsection::measurement_process}
The measurement process for the individual metrics is further split into specific user actions
which are executed and tested on both the iOS and Flutter app separately. These were chosen
to test all relevant facets of the app (see Section \ref{section:feature_presentation}) and ensure a necessary load on the system:
\begin{itemize}
    \item \textbf{app start:} The app is freshly installed on the test device, opened and idle until the visible postings are loaded.
    \item \textbf{scrolling:} On the postings overview screen, the posting cards are scrolled fully to the bottom and subsequently back to the beginning.
    \item \textbf{detail view:} From the postings overview, the first posting is tapped to navigate to the detail view. Afterwards the back button is tapped to navigate back to the overview.
    \item \textbf{image gallery:} The image gallery of a posting is opened from the detail view of a posting and the first 10 images are viewed by swiping.
\end{itemize}
For each \textbf{user action}, the average of all values over time is recorded. This process is then
repeated 3 times and averaged. The exact number of experiment repetitions was chosen as a
tradeoff between marginal accuracy increase and additional experiment execution time.\\
Furthermore, 2 testing rounds are devised on separate devices. The iPhone 12 and iPhone
SE are chosen as the upper and lower bounds of hardware performance respectively. The lower
bound is defined in this case as per Apples recommendation to set the deployment target to the
current operating system version (iOS 14 at time of writing) minus one (iOS 13) which lists the
iPhone SE as the oldest supported device (\cite{Apple2021}).\\
To reduce measurement bias, the device is restarted before each measurement to ensure that all
irrelevant background processes are cancelled.

\subsection{Profiling Tools} \label{subsection::profiling_tooling}
\textbf{Xcode Instruments} (\cite{Apple2019}) - a part of the \textbf{Xcode} IDE tool set - are used for profiling the individual metrics. It provides multiple preconfigured
profiling trace instruments.
For the purposes of this thesis, the \textbf{Time Profiler} tool (see Figure ...) is used for CPU (ref ...), the \textbf{Core Animation} tool (see Figure ...) for FPS (ref ...), and 
\textbf{Allocations} tool (see Figure ...) for memory usage (ref ...) quantification (see ) over time.


\subsection{Evaluation Process} \label{subsection::evaluation_process}
To better understand the data gathered, it is subsequently examined using exploratory data
analysis (EDA) (\cite{Tukey1977}). \textit{To be continued a bit...}


\section{User Experience Comparison} \label{section::usability_comparison_design}
%%% Goal %%%
This Section explores the usability hypothesis evaluation methodology. Specifically, laying out
the procedure to answer the question of whether or not the Flutter framework is capable of
reproducing native iOS application user experiences (see Section \ref{section::thesis_objective}).

%%% Problem with measuring UX and why performance results are used %%%
Generally, as UX is built for other humans, any evaluation is prone to subjectivity and perception
biases (\cite{Tversky1974}). Therefore, it is difficult to capture these impressions quantitatively in a reproducable manner.\\
However, the user experience of an app is directly dependant upon sufficiently underlying performance (e.g. for scrolling
fluidity). Therefore the results of the performance comparison (Section \ref{section::performance_comparison}) form the basis of the evaluation of the usability hypothesis $H_U$.
Utilizing a mixed approach as a study methodology combining both the quantitative performance comparison and a qualitative method will draw upon the strengths of both approaches. 

%%% Why I'm using expert interviews %%%
Specifically, semi structured interviews with subject matter experts (Cf. Vgl. Liebold/Trinczek 2009, S. 32 ff.) are conducted to evaluate the baseline application
with the Flutter replica by asking the participants for differences between the two apps (further explained in Subsection \ref{subsection::interview_guideline}). This methodology has the
advantage of covering predetermined topics relevant to the research question while also allowing spontaneous discussion possibly leading to novel insights.\\
Furthermore, expert interviews are an especially useful approach for scientific explorations
with no or scant preexisting theory (cf. \cite{Experts2009}).

%%% Number of interviews conducted %%%
The number of interviews conducted is limited as soon as no new perspectives seem to emerge from performing further interviews (Cf. ...).


\subsection{Interview Preliminaries and Technicalities}
The interviews are conducted with employees of apploft. They qualify as subject matter experts in the sense that they have been working in the mobile app industry for multiple years. They come from variety
of professional backgrounds including UX and UI design, project management as well as software engineering. Furthermore, some interviewees have actually worked on the original app itself. This diversity 
among the study participants is especially relevant in order to explore a breadth of perspectives (Cf. ...). 
The interviews are moderated and recorded by the author with a single interviewee per interview.
For the comparison, the interviewees receive QR codes with which both apps may be downloaded. Behind each distributed code is a downloadable IPA (iOS App Store Package) binary executable file hosted by an HTTP server.
These work exactly the same as any other apps downloaded from the iOS App Store.
Due to the ongoing Covid-19 pandemic, the interviews are conducted through video calls and the interviewees are asked to share their iPhone screen via Quicktime player (cite ...).

\subsection{Interview Guideline} \label{subsection::interview_guideline}
%% Goal %%%
The interview guideline (see Appendix ...) is based on finding out perceptual differences between the iOS baseline and Flutter replica app. 

%% Use Cases %%
Just like the performance comparison, use cases associated with particular UI features form the basis for the evaluation: 
\begin{itemize}
    \item \dots
    \item \dots
\end{itemize}
The interviewees are asked to perform a particular use case for app A and app B. Then they are asked to detail differences between the two apps.
Asking this open-ended question aims at receiving as much information possible about perceptual differences (Helfferich 2011, 182-185).
Subsequently, the participant is asked to determine which of the two apps felt more natural (i.e. had a better UX). A determined tertiary response of: "A", "B" or "same" is expected. 
The goal of this question is to get overall impression of the usability. \\
Both questions are asked after each use case execution of the participant. 
To maintain a high participant engagement during interviews, use cases are described in a more captivating way, e.g. "Please find the blue Mercedes SUV in Kickdown A [Wait until participant has found it.]. Now, 
please look for the black BMW convertible in the other app.". 
After each use case, the ordering of app A and B is swapped. E.g. if the first case starts with A, the second starts with B. This choice is made as to avoid recency bias (cite ...).
Furthermore, the ordering is also swapped after each interview. In this way, participant X starts with app A while participant Y starts with B.
Finally, after the last use case, the participant is asked to answer the two questions with regard to the entire application.


\subsection{Interview Evaluation}
The videos from the interviews are transcribed into a textual format and further processed using \textbf{interview coding} (cite ..). Thereby, each interview is categorized into semantic themes. These themes among
all interviews are then merged into an overall theme structure - also known as code structure. This code structure is forms the basis for the evaluation of the usability hypothesis $H_U$. 
% Todo: Further research this topic.